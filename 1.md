### 第17章 开源大模型ChatGLM高级定制化应用实战

在前一章中，我们介绍了ChatGLM的基本内容，并介绍了ChatGLM的一些常用应用，例如使用ChatGLM完成一些基本的场景问答以及特定信息抽取等。

除此之外，ChatGLM还可以完成一些更高级的任务和应用，例如通过提供的文档完成基于特定领域文档内容的知识问答等。不同类型的本地知识库可能对应不同的应用场景。

例如，在问答系统中，可以使用维基百科或其他在线百科全书作为本地知识库，以便回答用户的常见问题。在客服对话系统中，可以使用公司内部的产品文档或常见问题解答作为本地知识库，以便回答用户关于产品的问题。在聊天机器人中，可以使用社交媒体数据、电影评论或其他大规模文本数据集作为本地知识库，以便回答用户的聊天话题。



### 17.1 医疗问答GLMQABot搭建实战——基于ChatGLM搭建专业客服问答机器人

我们在16.6.2节已经介绍了使用基本的ChatGLM完成知识问答的一些方法，即直接通过Prompt将需要提出的问题传送给ChatGLM。可以看到ChatGLM已经能够较好地完成相关内容的知识问答，如图17 - 1所示。

#### 17.1.1 基于ChatGLM搭建专业领域问答机器人的思路

在使用ChatGLM制作专业领域问答机器人之前，我们需要了解ChatGLM能否完整地回答使用者所提出的问题。下面提出一个专业医学问题交于ChatGLM回答，代码如下：

![image](https://github.com/user-attachments/assets/858bfb7e-3fd4-4686-8995-176cab99fe8a)


```python
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half().cuda()

prompt_text = "小孩牙龈肿痛服用什么药"

print("普通ChatGLM询问结果: ")
response, _ = model.chat(tokenizer, prompt_text, history=[])
print(response)
```
这是一份最常见的生活类医学问答，问题是“小孩牙龈肿痛服用什么药”，在这里我们使用已有的ChatGLM完成此问题的回答，结果如图17 - 2所示（注意，在使用ChatGLM回答问题时，结果会略有不同）。

**普通chatGLM询问结果：**

The dtype of attention mask (torch.int64) is not bool

小孩牙龈肿痛的症状通常建议服用清热解毒的中药，例如清热解毒口服液、消炎咽喉颗粒、板蓝根颗粒等。这些中药具有清热解毒、抗菌消炎的功效，有助于缓解牙龈肿痛等症状。

同时，也可以让孩子多喝水，保持口腔清洁，避免食用辛辣刺激性食物，观察一段时间，看是否有改善。如果症状严重或持续时间较长，建议就医，寻求医生的帮助和建议。

这是一个较经典的回答，其中涉及用药建议，但是并没有直接回答我们所提出的问题，即“服用什么药”。专业回答建议如图17 - 3所示。

**专业回答建议**

[此处应是专业回答内容，但文档未完整呈现]

![image](https://github.com/user-attachments/assets/05f28860-5906-4dde-98a5-af96bafb14e9)


其中灰底部分是对这个问题的回答，即通过服用牛黄解毒丸可以较好地治疗小孩牙龈肿痛。这是一种传统的治疗方案。我们的目标就是希望ChatGLM能够根据所提供的文本资料回答对应的问题，而问题的答案应该就是由文本内容所决定的。

下面我们分析使用ChatGLM根据文本回答问题的思路。一个简单的办法就是将全部文档发送给ChatGLM，然后通过Prompt的方式告诉ChatGLM需要在发送的文档中回答特定的问题。

显然这个方法在实战中并不可信。首先，需要发送的文档内容太多，严重地消耗硬件的显存资源；其次，庞大的数据量会严重拖慢ChatGLM的回答；再次数据量过大还会影响ChatGLM查询文档的范围。

因此，我们需要换一种思路来完成实战训练。如果只发送与问题最相关的“部分文档”信息给ChatGLM，是否可行呢？整体流程如图17 - 4所示。


[此处应是图17 - 4整体流程内容，但文档未完整呈现]

![image](https://github.com/user-attachments/assets/e1d8df88-9210-48b1-9e75-76f46669cab5)


这只是一个思路，具体是否能够成功还需要读者自行尝试。

#### 17.1.2 基于真实医疗问答的数据准备

由于此项目是完成专业领域的问答，因此这里准备了一份真实的医疗问答实例作为基础数据，数据是根据具有实际意义的医学问答病例所设计的医疗常识，如图17 - 5所示。

[此处应是图17 - 5相关医疗常识内容，但文档未完整呈现]

![image](https://github.com/user-attachments/assets/0809602d-e7c0-4703-b194-1f413702f2a9)


下面对数据进行处理，在这里由于读取的文档内容是以JSON格式存储的，因此读取此内容的代码如下：
```python
import json
# 打开文件，r用于读取，encoding用于指定编码格式
with open('./dataset/train1.json', 'r', encoding='utf-8') as fp:
    # load()函数将fp(一个支持.read()的文件类对象，包含一个JSON文档)反序列化为一个Python对象
    data = json.load(fp)
    for line in data:
        line = (line["context_text"])  #获取文档中的context_text内容
        context_list.append(line)  #将获取到的文档添加到对应的list列表中
```
注意，本例中我们采用医疗问答数据作为特定的文档目标，读者可以选择自定义的专业领域文档或者内容作为特定目标进行处理。

#### 17.1.3 文本相关性（相似度）的比较算法

根据17.1.1节讲解的内容，在获取到对应的文档内容后，一个非常重要的工作是用特定的方法或者算法找到与提出的问题最相近的那部分答案。因此，这里的实战内容就转化成文本相关性（相似度）的比较和计算。

对于文本相关性的计算，相信读者不会陌生，常用的是余弦相关性计算与BM25相关性计算，在这里我们采用BM25来计算对应的文本相关性。

假如我们有一系列的文档Doc，现在要查询问题Query。BM25的思想是，对Query进行语素解析，生成语素Q；然后对于每个搜索文档Di计算每个语素Qi与文档Di的相关性；最后将所有的语素Qi与Di进行加权求和，最终计算出Query与Di的相似性得分。将BM25算法总结如下：

\[ Score(Query, D_i)=\sum_{i}^{n}W_i\cdot R(Q_i, D_i) \]

在中文中，我们通常将每一个词语当作Qi，Wi表示语素Qi的权重，R(Qi, Di)表示语素Qi与文档Di的相关性得分关系。

限于篇幅，对于BM25不再深入说明，有兴趣的读者可以自行研究。

下面通过程序实现BM25工程，我们可以通过编写自己的Python代码来实现BM25函数。对于成熟的算法，建议使用Python中现成的库函数，这是因为大多数现成的Python库，已经经过持续优化，我们没必要再重复制造轮子，使用现成的库即可。

读者可以使用如下代码安装对应的Python库：

![image](https://github.com/user-attachments/assets/2fa803d4-e96e-4f48-98eb-e2f190aff3c6)


```
pip install rank_bm25  #注意下画线
```
这是一个较常用的BM25库，其作用是计算单个文本与文本库的BM25值。但是需要注意的是，BM在公式中要求传递的是单个字（或者词），其过程是以单个字或者词为基础进行计算，因此在使用BM进行相关性计算时，需要将其拆分为字或者词的形式，完整的相关性计算代码如下：

```python
#query是需要查询的文本，documents为文本库，top_n为返回最接近的n条文本内容
def get_top_n_sim_text(query: str, documents: List[str], top_n = 3):
    tokenized_corpus = []
    for doc in documents:
        text = []
        for char in doc:
            text.append(char)
        tokenized_corpus.append(text)

    bm25 = BM25Okapi(tokenized_corpus)
    tokenized_query = [char for char in query]
    #doc_scores = bm25.get_scores(tokenized_query)  # array([0., 0.93729472, 0.])

    results = bm25.get_top_n(tokenized_query, tokenized_corpus, n=top_n)
    results = ["".join(res) for res in results]
    return results
```

对于此部分的代码应用如下：

```python
import utils

prompt_text = "明天是什么天气"
context_list = ["哪个颜色好看","今天晚上吃什么","你家电话多少","明天的天气是晴天","晚上的月亮好美呀"]
sim_results = utils.get_top_n_sim_text(query=prompt_text,documents=context_list, top_n=1)
print(sim_results)
```

最终打印结果如下：

\[ ['明天的天气是晴天'] \]

更多的内容请读者自行尝试。

当然，对于文本相似性的比较，除了使用BM25相关性计算外，还可以使用余弦相似度、欧拉距离以及深度学习的方法来实现，具体采用哪种方案，需要读者在不同的任务场景下比较选择。


#### 17.1.4 提示语句Prompt的构建

本小节使用基于专业文档搭建的GLMQABot问答机器人。

我们的目标是将相关的文本内容传递给ChatGLM，并显式地要求ChatGLM根据文档内容回答对应的问题，因此一个非常重要的内容就是显式地传递需要ChatGLM的Prompt。

在这里准备了一个可供ChatGLM使用的专门用于读取专业文档的Prompt，其内容如下：

```python
prompt = f'根据文档内容来回答问题，问题是"{question}"，文档内容如下：\n'
```

可以看到，此次任务的Prompt就是使用自定义的问题和查找到的相关内容组成一条特定的语句，要求ChatGLM对此语句做出回应。完整的构建Prompt的函数如下：

```python
def generate_prompt( question: str, relevant_chunks: List[str]):
    prompt = f'根据文档内容来回答问题，问题是"{question}"，文档内容如下：\n'
    for chunk in relevant_chunks:
        prompt += chunk + "\n"
    return prompt
```

#### 17.1.5 基于单个文档的GLMQABot的搭建

下面完成GLMQABot的问答搭建，按照17.1.1节的分析，现在只需要将所有的内容串联在同一个文件中即可，代码如下：

```python
import utils

context_list = []
import json
# 打开文件，r用于读取，encoding用于指定编码格式
with open('./dataset/train1.json', 'r', encoding='utf-8') as fp:
    # load()函数将fp(一个支持.read()的文件类对象，包含一个JSON文档)反序列化为一个Python对象
    data = json.load(fp)
    for line in data:
        line = (line["context_text"])
        context_list.append(line)

from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half().cuda()

prompt_text = "小孩牙龈肿痛服用什么药"

print("普通ChatGLM询问结果: ")
response, _ = model.chat(tokenizer, prompt_text, history=[])
print(response)
print("------------------------------")
print("经过文本查询的结果如下: ")
sim_results = utils.get_top_n_sim_text(query=prompt_text,documents=context_list)
print(sim_results)
print("------------------------------")
print("由ChatGLM根据文档查询的结果如下: ")
prompt = utils.generate_prompt(prompt_text,sim_results)
response, _ = model.chat(tokenizer, prompt, history=[])
print(response)
print("------------------------------")
```
在这里分别展示了不同情况下对问题的回答情况，下面说明一下结果。

**普通ChatGLM询问结果**

The dtype of attention mask (torch.int64) is not bool

小孩牙龈肿痛的情况，最好先咨询医生的建议。因为牙龈肿痛可能是由多种原因引起的，包括感染、牙齿问题等等，需要针对具体情况进行诊断和治疗。

如果医生建议就医，可以考虑以下药物：

1. **口腔漱口水**：口腔漱口水可以清洁口腔，减轻牙龈疼痛和炎症。例如，含有苯海拉明的漱口水可以缓解牙龈疼痛和炎症。

2. **消炎药**：如果医生认为牙龈肿痛是由感染引起的，可能会建议使用消炎药。例如，阿莫西林克拉维酸钾可以用于治疗细菌感染。

3. **止痛药**：如果医生认为牙龈肿痛是由疼痛引起的，可能会建议使用止痛药。例如，布洛芬可以缓解轻度到中度的疼痛。

需要注意的是，这些药物应该按照医生的建议使用，不要自行给孩子服用。此外，如果孩子的疼痛症状非常严重或持续时间较长，建议及时就医。

直接使用BM25进行文本查询的结果如图17 - 7所示。（此处图17 - 7内容文档未完整呈现 ） 

![image](https://github.com/user-attachments/assets/3876e4fa-c253-42c6-804a-4052f3405bb4)
