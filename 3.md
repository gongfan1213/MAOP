同样的编码方式。完整的相关性比较代码如下：
```python
import numpy
from utils import mean_pooling,compute_sim_score
from transformers import BertTokenizer, BertModel
import torch
embedding_model_name = "shibing624/text2vec-base-chinese"
# Load model from HuggingFace Hub
tokenizer = BertTokenizer.from_pretrained(embedding_model_name)
model = BertModel.from_pretrained(embedding_model_name)

query = ["雅生活服务的人工成本占营业成本的比例是多少"]
# 转化成Token
query_input = tokenizer(query, padding=True, truncation=True, return_tensors='pt')
# 计算输入Query的Embeddings
with torch.no_grad():
    model_output = model(**query_input)
# Perform pooling. In this case, mean pooling.
query_embedding = mean_pooling(model_output, query_input['attention_mask'])
print(query_embedding.shape)

import numpy as np
sentence_embeddings_np = np.load("./dataset/financial_research_reports/sentence_embeddings_np.npy")

for i in range(len(sentence_embeddings_np)):
    score = compute_sim_score(sentence_embeddings_np[i],query_embedding[0])
    print(i,score)
```

从代码来看，首先将文本转换成Token格式，之后使用模型计算Query的Embedding编码，最后将查询Query的Embedding编码与上一步存档的Embedding编码内容进行相关性计算。

前面使用BM25完成了文本的相关性计算，并且提供了相关性计算的公式。可以认为BM25是前面介绍的TF - IDF的一个变种，它基于字符出现的频率计算不同文本之间的相关性。

我们现在需要计算的目标并不是单独的一系列的长字符，更倾向于对文本内容的语义理解，因此在这个阶段的相关性计算上，需要使用一种计算方法来计算涉及语义的文本相关性问题。

对于文本相似度计算，研究人员在自然语言处理中提出了多种解决方案，其中最常用的是文本的余弦相似度计算，公式如下：

\[ similarity(A,B)=\frac{A\cdot B}{||A||\times||B||}=\frac{\sum_{i = 1}^{n}A_i\times B_i}{\sqrt{\sum_{i = 1}^{n}A_i^2}\times\sqrt{\sum_{i = 1}^{n}B_i^2}} \]

公式中A和B分别代表文本A和文本B构成的编码向量，后续通过计算这个编码向量的值，从而计算出两个文本之间的相关性（相似度）。其实现代码如下：

![image](https://github.com/user-attachments/assets/d626591b-0fe4-4b50-985b-ed9dc1544669)


```python
import numpy as np
def compute_sim_score( v1: np.ndarray, v2: np.ndarray) -> float:
    return v1.dot(v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
```
对查询文本和存储文本进行相关性比较的完整代码如下：
```python
import numpy
from utils import mean_pooling,compute_sim_score
from transformers import BertTokenizer, BertModel
import torch
embedding_model_name = "shibing624/text2vec-base-chinese"
# Load model from HuggingFace Hub
tokenizer = BertTokenizer.from_pretrained(embedding_model_name)
model = BertModel.from_pretrained(embedding_model_name)

query = ["雅生活服务的人工成本占营业成本的比例是多少"]
# 对文本进行编码
query_input = tokenizer(query, padding=True, truncation=True, return_tensors='pt')
# 使用模型进行推断
with torch.no_grad():
    model_output = model(**query_input)
# 对推断结果进行均值池化压缩
query_embedding = mean_pooling(model_output, query_input['attention_mask'])
print(query_embedding.shape)

import numpy as np
sentence_embeddings_np = np.load("./dataset/financial_research_reports/sentence_embeddings_np.npy")

#依次计算不同的句向量相似度
for i in range(len(sentence_embeddings_np)):
    score = compute_sim_score(sentence_embeddings_np[i],query_embedding[0])
    print(i,score)
```
最终打印结果如图17 - 12所示。

**图17 - 12 最终打印结果**

```js
torch.Size([1, 768])
0 0.4350432
1 0.60119087
2 0.45916626
3 0.46189007
4 0.48611978
5 0.4371616
6 0.37968332
7 0.52807575
8 0.43902677
9 0.4859489
```
![image](https://github.com/user-attachments/assets/12c3a7dd-7027-436b-af66-4febd4aa7a91)


这里只截取了部分结果，可以看到前两名的分数排名分别是研报1和研报7，在读取前期对研报1和研报7的内容都进行了阅读，发现研报1和研报7虽然都涉及我们所查询的目标，但是其侧重点不同，相对于研报7的内容，研报1包含更多我们所需要的目标。

### 17.2.4 基于知识链的ChatGLM本地化知识库检索与智能答案生成

相信读者经过前面的分析，对于在多个文档中查找最相关的文档有了一定的了解。接下来将查找到的相关文档重新进行文本比对，将与问题最相关的若干条文档内容输入ChatGLM中，阅读并反馈查询问题的答案。

这部分工作较简单，读者可以参考17.1节的讲解完成这部分内容的学习。需要注意的是，我们查询的问题可能并不存在于输入的文本内容中，对于某些问题，ChatGLM会根据以往的训练内容自动生成答案，但是这个答案可能并不是我们想要的，只是依靠文档内容得到的。因此，在这里创建相关的Prompt时，需要显式地告诉ChatGLM不可以凭借经验或者以往的训练内容来回答问题。

Prompt的设置如下：

```python
prompt = f'严格根据文档内容来回答问题，回答不允许编造成分要符合原文内容，问题是"{question}"，文档内容如下：\n'
```
下面给出基于知识链的ChatGLM本地化知识库检索与智能答案生成模型的完整代码。

**knowledge_chain_chatGLM_step0代码如下：**


```python
kg_vector_stores = {
    '大规模金融研报': './dataset/financial_research_reports',
    '初始化': './dataset/cache',
}  # 可以替换成自己的知识库，如果没有，则需要设置为None

from transformers import BertTokenizer, BertModel
import torch
embedding_model_name = "shibing624/text2vec-base-chinese"
embedding_model_length = 512  #对于任何长度的文本，都有一个长度限制
# Mean Pooling - Take attention mask into account for correct averaging
def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0]  #First element of model_output contains all token embeddings
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

# Load model from HuggingFace Hub
tokenizer = BertTokenizer.from_pretrained(embedding_model_name)
model = BertModel.from_pretrained(embedding_model_name)

sentences = []
#这里先对每个文档进行读取
import os
path = "./dataset/financial_research_reports/"
filelist = [path + i for i in os.listdir(path)]
for file in filelist:
    if file.endswith(".txt"):
        with open(file,"r",encoding="utf-8") as f:
            lines = f.readlines()
            lines = "".join(lines) #注意这里要做成一个文本
            sentences.append(lines[:embedding_model_length])

# Tokenize sentences
encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')

# Compute token embeddings
with torch.no_grad():
    model_output = model(**encoded_input)
# Perform pooling. In this case, mean pooling.
sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])
print("Sentence embeddings:")
print(sentence_embeddings)
print(sentence_embeddings.shape)

import numpy as np
sentence_embeddings_np = sentence_embeddings.detach().numpy()
np.save("./dataset/financial_research_reports/sentence_embeddings_np.npy",sentence_embeddings_np)
```

**knowledge_chain_chatGLM_step1代码如下：**


```python
import numpy
from utils import mean_pooling,compute_sim_score
from transformers import BertTokenizer, BertModel
import torch
embedding_model_name = "shibing624/text2vec-base-chinese"
# Load model from HuggingFace Hub
tokenizer = BertTokenizer.from_pretrained(embedding_model_name)
model = BertModel.from_pretrained(embedding_model_name)

query = ["雅生活服务的人工成本占营业成本的比例是多少"]
# Tokenize sentences
query_input = tokenizer(query, padding=True, truncation=True, return_tensors='pt')
# Compute token embeddings
with torch.no_grad():
    model_output = model(**query_input)
# Perform pooling. In this case, mean pooling.
query_embedding = mean_pooling(model_output, query_input['attention_mask'])
print(query_embedding.shape)

import numpy as np
sentence_embeddings_np = np.load("./dataset/financial_research_reports/sentence_embeddings_np.npy")

#依次计算不同的句向量相似
for i in range(len(sentence_embeddings_np)):
    score = compute_sim_score(sentence_embeddings_np[i],query_embedding[0])
    print(i,score)
```

**knowledge_chain_chatGLM_step2代码如下：**


```python
import utils
query = ["雅生活服务的人工成本占营业成本的比例是多少"]

context_list = []
with open("./dataset/financial_research_reports/yanbao001.txt","r",encoding="UTF-8") as f:
    lines = f.readlines()
    for line in lines:
        line = line.strip()
        context_list.append(line)

print("经过文本查询的结果如下: ")
sim_results = utils.get_top_n_sim_text(query=query[0],documents=context_list)
print(sim_results)
print("-----------------------------------")

from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half().cuda()

print("由ChatGLM根据文档的严格回答的结果如下: ")
prompt = utils.strict_generate_prompt(query[0],sim_results)
response, _ = model.chat(tokenizer, prompt, history=[])
print(response)
print("-----------------------------------")
```
经过文本查询的结果如图17 - 13所示。

**图17 - 13 经过文本查询的结果**

[此处应是具体文本查询结果内容，但文档未完整呈现  ]

如果要求ChatGLM完全依靠文档内容对问题进行回答，输出的结果如图17 - 14所示。

![image](https://github.com/user-attachments/assets/d0c76b50-b2e7-4102-888b-2f96579904be)


**图17 - 14 输出结果**

由chatGLM根据文档的严格回答的结果如下：

The dtype of attention mask (torch.int64) is not bool

根据文档内容，雅生活服务的人工成本占营业成本的比例约为61.00%。 


更多查询的问题请读者自行尝试。 
