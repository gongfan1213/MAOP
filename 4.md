### 17.3 基于ChatGLM的一些补充内容

除了前面两节介绍的ChatGLM的知识链内容外，ChatGLM还可以完成更多的功能，如图形生成、图片描述、音频生成等。具体内容还请读者自行挖掘和研究。

#### 17.3.1 语言的艺术——Prompt的前世今生

前面章节较少涉及Prompt，本小节将着重介绍此方面的内容。

通过前面的演示，读者应该对Prompt有了一定的了解，即通过输入特定的“语言组合”，使得模型能够更好地适配各种任务。因此，合适的Prompt对于模型的效果至关重要。大量研究表明，Prompt的微小差别可能会造成效果的巨大差异。研究者就如何设计Prompt做出了各种各样的努力，如自然语言背景知识的融合、契合目标的约束条件、不再拘泥于语言形式的Prompt探索等，如图17 - 15所示。


![image](https://github.com/user-attachments/assets/8b58d761-bb91-474b-bdc6-7a0e24ac5538)


**图17 - 15 研究者就如何设计Prompt做出了各种努力**

[此处是一幅相关的简笔画，但文档未对画中内容进行详细文字描述  ]


Prompt刚刚出现的时候，还不叫作Prompt，它只是研究人员为了下游任务设计出来的一种输入形式或模板，它能够帮助语言模型“回忆”起自己在预训练时“学习”到的东西，因此逐渐被称呼为Prompt。

例如，在对电影评论进行二分类的时候，最简单的提示模板是“. It was [mask]. ”，但是该模板并没有突出该任务的具体特性，我们可以为其设计一个能够突出该任务特性的模板，例如“The movie review is. It was [mask]. ”，然后根据mask位置的输出结果映射到具体的标签上。

可以看到，在实际应用中，用户最常用到的就是Prompt设计，其设计需要考虑以下两项内容：
- Prompt的适配。
- 设计Prompt模板。


**1. Prompt的适配**

Prompt的形状主要指的是任务的目标和类型。Prompt在实际应用中选择哪一种，主要取决于任务的形式和模型的类别。一般来说，完形填空类型的Prompt和遮蔽语言模型的训练方式非常类似，因此，对于使用遮蔽语言模型的任务来说，完形填空类型的Prompt更加合适；对于生成任务来说，或者使用自回归语言模型解决的任务，带有提问的Prompt更加合适；全文生成类型的Prompt较为通用，因此对于多种任务，全文生成类型的Prompt均适用。另外，对于文本分类任务，Prompt模板通常采用的是问答的形式。


**2. 设计Prompt模板**

Prompt最开始就是从手工设计模板开始的。模板设计一般基于人类的自然语言知识，力求得到语义流畅且高效的模板。由于大型语言模型在预训练过程中见过了大量的人类世界的自然语言，很自然地受到了影响，因此在设计模板时需要符合人类的语言习惯。

对于Prompt的研究和探索是基于自然语言处理大模型发展而来的，其主要作用是引导模型的内容生成，先定角色，后说背景，再提要求，最后定风格。这是Prompt设计和使用的基本方法，展开详细介绍的话就是另一本实战方面的书籍了，因此这里不再过多阐述，相信读者后续会持续与Prompt进行交流，从而掌握更多的使用方法和技巧。



#### 17.3.2 清华大学推荐的ChatGLM微调方法

本小节使用Linux系统完成对ChatGLM的微调，注意这里只简单介绍，更多微调相关内容请参考第18章。

在学习了使用ChatGLM进行文本问答或者文本抽取后，相信读者一定想要尝试更多的场景，一个非常简单的想法是利用其生成一些广告文案来辅助我们的日常工作。

但是问题在于，对于部分特定的文本生成或者问答，原有的ChatGLM由于没有学习过对应的内容，因此在生成的时候可能并不会生成所需要的目标文档。为了解决这个问题，ChatGLM官方给我们提供了对应的微调方案，P - Tuning就是一种对预训练语言模型进行少量参数微调的技术。

所谓预训练语言模型，是指在大规模的语言数据集上训练好的、能够理解自然语言表达并从中学习语言知识的模型。P - Tuning所做的就是根据具体的任务对预训练的模型进行微调，让它更好地适应具体的任务。相比于重新训练一个新的模型，微调可以大大节省计算资源，同时也可以获得更好的性能表现。在这里，读者运行本书配套源码包/第十七章/ptuning目录中对应的文件train.sh，如图17 - 16所示。

![image](https://github.com/user-attachments/assets/1dbb616a-27f5-4742-8c92-574c4402ae4a)



**图17 - 16 train.sh**

[显示了相关目录结构，包含dataset、knowledge_chain_chatGLM、ptuning等文件夹，ptuning文件夹下列出了arguments.py、deepspeed.json等文件  ]

并运行以下指令即可：
```bash
bash train.sh
```
可以将train.sh中的train_file、validation_file和test_file修改为自己的JSON格式数据集路径，并将prompt_column和response_column改为JSON文件中输入文本和输出文本对应的KEY。可能还需要增大max_source_length和max_target_length来匹配自己的数据集中的最大输入输出长度。


```bash
PRE_SEQ_LEN=32
LR=2e-2

CUDA_VISIBLE_DEVICES=0 python3 main.py \
    --do_train \
    --train_dir train.json \
    --validation_file dev.json \
    --prompt_column content \
    --response_column summary \
    --overwrite_cache \
    --model_name_or_path /mnt/workspace/chatglm-6b \
    --output_dir output/adgen-chatglm-6b-pt-$PRE_SEQ_LEN-$LR \
    --overwrite_output_dir \
    --max_source_length 128 \
    --max_target_length 128 \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 1 \
    --gradient_accumulation_steps 16 \
    --predict_with_generate \
    --max_steps 3000 \
    --logging_steps 10 \
    --save_steps 1000 \
    --learning_rate $LR \
    --pre_seq_len $PRE_SEQ_LEN
```

train.sh中的PRE_SEQ_LEN和LR分别表示Soft Prompt的长度和训练的学习率，可以进行调节以取得最佳效果。P - Tuning - v2方法会冻结全部的模型参数，可通过调整quantization_bit来控制原始模型的量化等级，若不加此选项，则使用FP16精度加载模型。

在默认配置quantization_bit=4、per_device_train_batch_size=1、gradient_accumulation_steps=16的条件下，INT4的模型参数被冻结，一次训练迭代会以1批的处理大小进行16次累加的前后向传播，等效为16的总批处理大小，此时最低只需6.7GB显存。若想在同等批处理大小下提升训练效率，则可在二者乘积不变的情况下加大per_device_train_batch_size的值，但这样会带来更多的显存消耗，请根据实际情况酌情调整。

至于使用模型进行推理，在这里官方同样给我们提供了对应的代码，在推理时需要同时加载原ChatGLM - 6B模型以及PrefixEncoder的权重，可以使用evaluate.sh脚本：

```bash
bash evaluate.sh
```
其中要特别注意参数部分，此时需要载入训练后的参数：
```bash
--model_name_or_path THUDM/chatglm-6b
--ptuning_checkpoint $CHECKPOINT_PATH
```
最后还要提醒一下，这里的微调是基于Linux操作系统的，更详细的微调方案将在第18章详细讲解。

#### 17.3.3 一种新的基于ChatGLM的文本检索方案
首先来回顾一下前面是如何基于ChatGLM进行文档问答的，其中心思路是“先检索，再整合”，大致思路如下：

- 首先准备好文档，把每个文档切成若干小的模块。

- 调用文本转向量的接口，将每个模块转为一个向量，并存入向量数据库。

- 当用户发来一个问题的时候，将问题同样转为向量，并检索向量数据库，得到相关性最高的一个模块。

- 将问题和检索结果合并重写为一个新的请求发给ChatGLM进行文档问答。



这里实际上是将用户请求的Query和Document进行匹配，也就是所谓的问题 - 文档匹配。问题 - 文档匹配的问题在于问题和文档在表达方式上存在较大差异。

通常Query以疑问句为主，而Document则以陈述说明为主，这种差异可能会影响最终匹配的效果。一种改进方法是，跳过问题和文档匹配部分，先通过Document生成一批候选的问题 - 答案匹配，当用户发来请求的时候，首先是把Query和候选的Question进行匹配，进而找到相关的Document片段，此时的具体思路如下：

- 首先准备好文档，并整理为纯文本的格式，把每个文档切成若干个小的模块。

- 调用ChatGLM的API，根据每个模块生成5个候选的Question，使用的Prompt格式为“请根据下面的文本生成5个问题：……”。

- 调用文本转向量的接口，将生成的Question转为向量，存入向量数据库，记录Question和原始模块的对应关系。

- 当用户发来一个问题的时候，将问题同样转为向量，并检索向量数据库，得到相关性最高的一个Question，进而找到对应的模块。

- 将问题和模块合并重写为一个新的请求发给ChatGLM进行文档问答。


限于篇幅，这就不具体实现了，请读者自行尝试完成。

### 17.4 本章小结

本章介绍了ChatGLM的高级应用，即基于知识链的多专业跨领域文档挖掘的方法。这是目前有关ChatGLM甚至自然语言处理大模型领域最前沿的研究方向。除了本章中讲解的两个例子外，基于大模型的应用场景涵盖自然语言处理、计算机视觉、推荐系统、医疗健康、智能交通、金融服务等多个领域。

这些场景需要使用大规模的深度学习模型，并在大规模计算环境中进行训练和推理，以提高精度和效率。本章只是抛砖引玉，相信读者在学习完本章后，会对大模型的应用有进一步的了解，并且可以开发出更多基于深度学习大模型的应用。 
