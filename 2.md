### 17.1 医疗问答GLMQABot搭建实战——基于ChatGLM搭建专业客服问答机器人
（接上）

![image](https://github.com/user-attachments/assets/1058b15a-e75a-4cf7-b206-21046fcbe705)



**经过文本查询的结果如下：**

[“牙龈肿痛会给我们健康口腔带来很大伤害性，为此在这里提醒广大朋友们，对于牙龈肿痛一定要注意做好相关预防工作，牙龈肿痛的发病群体非常广泛，孩子以后收到此病害侵害，如果孩子不幸患上牙龈肿痛，需要尽快接受正规治疗，并且做好护理工作。牙龈肿痛又被称为了牙肉肿痛，也就是牙齿根部疼痛并且伴随着它的周围齿肉肿胀。小孩发生牙龈肿痛的原因比较多，不同病因引起的牙龈肿痛治疗方法也有差异，所以当孩子出现这种疾病之后，需要尽快去医院查明原因后对症处理。处理小孩牙龈肿痛的三个方法是什么1、牛黄解毒丸孩子出现牙龈肿痛多是因为上火造成的，这个时候可以给孩子服用牛黄解毒丸，按照医生的嘱托来给孩子服用。此药物有通便泻火的作用，而且是属于中药，对于孩子来说没有副作用，但一定要注意不可以大量给孩子服用。2、用温水刷牙孩子牙龈肿痛的时候一定要用温水刷牙，用温茶水来漱口。因为牙髓神经对于温度是非常敏感的，如果一旦遇到刺激就会加重疼痛。温水对于牙齿来说是天然保护剂，能够防治过敏性牙痛疾病。茶水面含氟，经常用温茶水含漱口，能够起到护齿防龋治疗牙痛的作用。3、大蒜头磨擦对于有比较严重磨损的牙齿，并且有明确酸痛区的孩子，家长朋友们可以用大蒜头反复去摩擦牙龈敏感区，每天磨擦两次，等一周之后酸痛感就会明显减轻。牙龈肿痛虽然说不是特别严重的疾病，但它的危害也比较大，为此在这里提醒广大家长朋友们，如果孩子一旦出现这种情况，就应该及时带着孩子去医院接受检查。因为牙龈肿痛发生的原因比较多，只有检查确诊什么原因后，才能对症治疗。’宝宝牙龈红肿出血可能是因为龈炎、牙周炎等疾病造成的，建议发现病情之后去医院做检查，了解宝宝病情是怎么回事。平时可以给宝宝喝一些绿豆汤、吃一些下火的蔬菜、多喝水，如果病情比较严重，那么就需要吃一些消炎药物，需要医生对症用药才可以恢复的。一岁左右的宝宝，由于消化系统与肠胃系统是在逐步完善的过程，日常吃进去的食物，很容易出现上火的情况。宝宝一旦上火，就会导致咳嗽，喉咙痛，牙龈红肿等情况，也是新手爸妈最担心的。那么宝宝牙龈红肿出血怎么办？1、宝宝牙龈红肿出血护理方法建议给孩子喝些绿豆汤或绿豆稀饭，绿豆性寒味甘，能清凉解毒，清热解烦，脾气暴躁、心烦意乱的宝宝最适宜。多给孩子吃些水果，如柚子、梨：性寒味微酸，除能清热外，其特点是能清润肺系，对于肺热咳嗽吐黄痰，咽干而痛的宝宝极适宜。多吃些清火蔬菜，如白菜：性微寒，能清热除烦。”]



**由ChatGLM根据文档查询的结果如下：**

小孩牙龈肿痛可以服用牛黄解毒丸或者用温水刷牙、用温茶水来漱口等方法。如果病情比较严重，需要吃一些消炎药物，建议宝宝喝一些绿豆汤、吃一些下火的蔬菜、多喝水，并去医院接受检查。注意消除紧张，可去口腔科检查，如果是乳牙萌出，出血是正常的。宝宝牙龈红肿的原因多见于牙菌斑生物膜引起的牙龈组织感染性疾病，也可见于局部异物刺激所致。牙龈健康是非常重要的，一旦保护不好，每天都会面临牙疼的问题，年纪大了牙齿也不会好的。


此时我们做一个对比，对于不同条件下的问答，可以很明显地看到，基于提供的文本内容ChatGLM作了较为全面的回复，即采用“牛黄解毒丸”作为最优的解答方案，而不是采用较为经典的回答来答复问题。

![image](https://github.com/user-attachments/assets/44bd7e5f-4d56-4117-ae02-99c00f0eca63)


### 17.2 金融信息抽取实战——基于知识链的ChatGLM本地化知识库检索与智能答案生成
#### 17.2.1 基于ChatGLM搭建智能答案生成机器人的思路

在构建知识链检索机器人之前，我们需要对总体环节进行设置，即构建基于知识链的ChatGLM本地化知识库检索与智能答案生成的机器人需要哪些步骤。

遵循人类的思维习惯，当一个较为专业的问题来临时，首先需要在所有的知识范畴或者知识库中查询所涉及的文档内容，之后阅读相关的文档，从而解析出对应的目标。一个完整的知识链问答流程如图17 - 9所示。

![image](https://github.com/user-attachments/assets/1bbad298-8d37-432a-89c3-4a989a9cf0e5)


**图17 - 9 完整的知识链问答流程**

[此处应是流程图具体内容，但文档未完整呈现相关描述 ，大致流程为从本地知识文件读取内容，文本切分后向量化，构建索引，输入Query并向量化，检索相关Document，拼接文本得到Context，填充Prompt模板，经LLM（ChatGLM - 6B）得到Response]


从图17 - 9可以看到，相对于17.1节构建的基于专业领域的问答机器人，这里的步骤明显更多。这里将本章的主要内容用浅色框标出，本节首先需要在广泛的文档中找到所要查询的特定文件，然后选择其中之一或者若干文档，通过17.1节的内容进行完整的推断。

![image](https://github.com/user-attachments/assets/ded68d7d-5892-4624-9cde-e284926097d7)


本节的目标是提供一条需要查询的语句：

```python
Query = ["雅生活服务的人工成本占营业成本的比例是多少"]
```
并且提供若干金融文档，希望读者阅读这些文档后回答这个简单的问题。提供的文档样式如图17 - 10所示。


**图17 - 10 提供的文档**

[列出了一系列文档名，如financial_research_report、sentence_embedding、yanbao0001.txt - yanbao011.txt  ]


为了增加本节的难度，为需要查询的内容额外添加了干扰文本，分别是yanbao001与yanbao007，读者可以预先查阅和比较。

#### 17.2.2 获取专业（范畴内）文档与编码存储

在17.2.1节的分析中，想要完成一条完整的知识链，首先要根据所提出的问题获取所有涉及的文档。对于此问题的回答，一个非常简单的答案就是将所有的文档“喂”给ChatGLM，之后根据问题要求其回答是否相关。

这样是可行的，但是在实际应用这种方法时一般会产生如下问题：

- 文档长度过长，无法一次性喂给ChatGLM读取。

- 文档数量过多，ChatGLM阅读花费的时间过多。

- 查询内容雷同，使得ChatGLM多次重复阅读相同内容，浪费成本。

- 第一次查询的结果无法存档。

- 产生的结果较为分散，无法聚焦于具体问题。

对于ChatGLM来说，一次性输入较多的文档内容会使得模型产生爆显存的问题；同时，如果文档内容过多，在一定时间内要求阅读过多的文本内容，则会花费大量ChatGLM的时间；再就是当查询的内容相同或类似时，会白白浪费查询成本。对于结果来说，由于ChatGLM本身具有一定的不确定性，对于第一次查询结果的输出可能并不是很确定，缺乏一个统一的标准，产生的结果往往较分散，并且无法对其进行统一标准的存储。因此，仅仅使用ChatGLM完成结果的输出并不合适。

而对于文档级别的文本抽取和比对，目前比较常用的方法是使用深度学习的文档级对比工具来完成，即首先传入文档信息，在文档编码成Embedding后，再将文档信息的编码存储下来，之后通过相关性算法对查询内容的编码和现有存储的编码进行比较。涉及的部分内容如图17 - 11所示。

![image](https://github.com/user-attachments/assets/eea75344-f949-4ab6-b97d-49b347502e8c)



**图17 - 11 涉及的部分内容**
[此处内容文档未完整呈现相关描述  ]

下面通过代码实现这部分内容，首先获取完整的文档内容，代码如下：


```python
from transformers import BertTokenizer, BertModel
import torch
embedding_model_name = "shibing624/text2vec-base-chinese"
embedding_model_length = 512  #对于任何长度的文本，模型在处理时都有一个长度限制

# Load model from HuggingFace Hub
tokenizer = BertTokenizer.from_pretrained(embedding_model_name)
model = BertModel.from_pretrained(embedding_model_name)

sentences = []
#这里先对每个文档进行读取
import os
path = "./dataset/financial_research_reports/"
filelist = [path + i for i in os.listdir(path)]
for file in filelist:
    if file.endswith(".txt"):
        with open(file,"r",encoding="utf-8") as f:
            lines = f.readlines()
            lines = "".join(lines) #注意这里要做成一个文本
            sentences.append(lines[:embedding_model_length])
```

需要注意的是，对于任何长度的文本，我们在使用模型进行处理时都有一个特定的输入长度，在这里选用长度为512的文本进行截断，读者可以根据自己的硬件条件以及对各种深度学习模型的熟悉程度选择不同的长度进行文本截断。

接下来使用深度学习模型对文本进行Embedding处理，这是一个较常用的文本Embedding化的工具。完整代码如下：

```python
kg_vector_stores = {
    '大规模金融研报': './dataset/financial_research_reports',
    '初始化': './dataset/cache',
}  # 可以替换成自己的知识库，如果没有，则需要设置为None

from transformers import BertTokenizer, BertModel
import torch
embedding_model_name = "shibing624/text2vec-base-chinese"
embedding_model_length = 512  #对于任何长度的文本，都有一个长度限制
# Mean Pooling - 对最终的Embedding结果进行压缩
def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0]  #First element of model_output contains all token embeddings
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

# 从HuggingFace服务器中加载模型
tokenizer = BertTokenizer.from_pretrained(embedding_model_name)
model = BertModel.from_pretrained(embedding_model_name)

sentences = []
#这里先对每个文档进行读取
import os
path = "./dataset/financial_research_reports/"
filelist = [path + i for i in os.listdir(path)]
for file in filelist:
    if file.endswith(".txt"):
        with open(file,"r",encoding="utf-8") as f:
            lines = f.readlines()
            lines = "".join(lines) #注意这里要做成一个文本
            sentences.append(lines[:embedding_model_length])

# 对文本进行编码
encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')

# 使用模型进行推断
with torch.no_grad():
    model_output = model(**encoded_input)

# 对推断结果进行均值池化压缩
sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])
print("Sentence embeddings:")
print(sentence_embeddings)
print(sentence_embeddings.shape)

import numpy as np
sentence_embeddings_np = sentence_embeddings.detach().numpy()
np.save("./dataset/financial_research_reports/sentence_embeddings_np.npy",sentence_embeddings_np)
```
可以清楚地看到，对于读取的文本文档内容，模型通常会先对其进行编码化处理，然后针对长度不同的文档内存，通过mean的方式将其压缩成同一幅度大小的Embedding编码，最后对生成的Embedding编码进行存储。

#### 17.2.3 查询文本编码的相关性比较与排序

本小节进行相关性文本的比较与排序。按照17.2.1节的分析可知，文本编码的目的是对输入的查询文本进行比较，计算其相关性排序，从而确定查询文本与已有文本的相关程度。

需要注意的是，这一阶段与上一阶段相似，也要对输入的查询文本内容进行编码，因此需要 （文档此处内容不完整 ） 
